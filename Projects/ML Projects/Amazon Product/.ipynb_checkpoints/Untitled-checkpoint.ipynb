{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c53a2d2-57fa-4b85-a9d3-5a23fec53801",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m URL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://amzn.in/d/cRBK2D8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Scrape data\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m product_data \u001b[38;5;241m=\u001b[39m scrape_amazon(URL)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Convert to DataFrame for better viewing and saving\u001b[39;00m\n\u001b[1;32m     38\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(product_data)\n",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m, in \u001b[0;36mscrape_amazon\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Scraping product details\u001b[39;00m\n\u001b[1;32m     17\u001b[0m products \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m product \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms-main-slot s-result-list s-search-results sg-row\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata-component-type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms-search-result\u001b[39m\u001b[38;5;124m\"\u001b[39m}):\n\u001b[1;32m     19\u001b[0m     title \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mh2\u001b[38;5;241m.\u001b[39ma\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     20\u001b[0m     price_whole \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma-price-whole\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define headers to mimic a browser visit\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US, en;q=0.9\",\n",
    "}\n",
    "\n",
    "# Function to scrape a single Amazon page\n",
    "def scrape_amazon(url):\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Scraping product details\n",
    "    products = []\n",
    "    for product in soup.find_all(\"div\", class_=\"s-main-slot s-result-list s-search-results sg-row\")[0].find_all(\"div\", {\"data-component-type\": \"s-search-result\"}):\n",
    "        title = product.h2.a.text.strip()\n",
    "        price_whole = product.find(\"span\", class_=\"a-price-whole\")\n",
    "        price_fraction = product.find(\"span\", class_=\"a-price-fraction\")\n",
    "        price = f\"{price_whole.text}.{price_fraction.text}\" if price_whole and price_fraction else \"N/A\"\n",
    "        rating = product.find(\"span\", class_=\"a-icon-alt\")\n",
    "        products.append({\n",
    "            \"Title\": title,\n",
    "            \"Price\": price,\n",
    "            \"Rating\": rating.text if rating else \"N/A\"\n",
    "        })\n",
    "    return products\n",
    "\n",
    "# URL of the Amazon page to scrape\n",
    "URL = \"https://amzn.in/d/cRBK2D8\"\n",
    "\n",
    "# Scrape data\n",
    "product_data = scrape_amazon(URL)\n",
    "\n",
    "# Convert to DataFrame for better viewing and saving\n",
    "df = pd.DataFrame(product_data)\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"amazon_products.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e65ad68b-33e6-47f9-9b65-ee64ab85e432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': 'Xiaomi Pad 6| Qualcomm Snapdragon 870| Powered by HyperOS |144Hz Refresh Rate| 8GB, 256GB| 2.8K+ Display (11-inch/27.81cm) Tablet| Dolby Vision Atmos| Quad Speakers| Wi-Fi| Gray', 'Price': '23,999.', 'Rating': '4.5 out of 5 stars'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US, en;q=0.9\",\n",
    "}\n",
    "\n",
    "def scrape_product_page(url):\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract product title\n",
    "    try:\n",
    "        title = soup.find(\"span\", id=\"productTitle\").text.strip()\n",
    "    except:\n",
    "        title = \"N/A\"\n",
    "\n",
    "    # Extract price\n",
    "    try:\n",
    "        price = soup.find(\"span\", class_=\"a-price-whole\").text.strip()\n",
    "    except:\n",
    "        price = \"N/A\"\n",
    "\n",
    "    # Extract rating\n",
    "    try:\n",
    "        rating = soup.find(\"span\", class_=\"a-icon-alt\").text.strip()\n",
    "    except:\n",
    "        rating = \"N/A\"\n",
    "\n",
    "    return {\n",
    "        \"Title\": title,\n",
    "        \"Price\": price,\n",
    "        \"Rating\": rating,\n",
    "    }\n",
    "\n",
    "# URL of the product page\n",
    "URL = \"https://amzn.in/d/cRBK2D8\"\n",
    "product_details = scrape_product_page(URL)\n",
    "print(product_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b2e7902-ee2e-4ca3-97ab-508791308f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product details saved to product_details.csv\n",
      "Reviews saved to product_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def extract_product_details(soup):\n",
    "    # Extract product name, price, description, and rating\n",
    "    product_name = soup.find(\"span\", {\"id\": \"productTitle\"})\n",
    "    product_name = product_name.get_text(strip=True) if product_name else \"No product name available\"\n",
    "    \n",
    "    price = soup.find(\"span\", {\"class\": \"a-price-whole\"})\n",
    "    price = price.get_text(strip=True) if price else \"Not Available\"\n",
    "    \n",
    "    description = soup.find(\"div\", {\"id\": \"feature-bullets\"})\n",
    "    description = description.get_text(strip=True) if description else \"No description available\"\n",
    "    \n",
    "    rating = soup.find(\"span\", {\"class\": \"a-icon-alt\"})\n",
    "    rating = rating.get_text(strip=True) if rating else \"No rating\"\n",
    "\n",
    "    return {\n",
    "        \"Product Name\": product_name,\n",
    "        \"Price\": price,\n",
    "        \"Description\": description,\n",
    "        \"Rating\": rating,\n",
    "    }\n",
    "\n",
    "def extract_reviews(soup):\n",
    "    reviews = []\n",
    "    \n",
    "    # Find all review items\n",
    "    review_elements = soup.find_all(\"li\", {\"data-hook\": \"review\"})\n",
    "    for review in review_elements:\n",
    "        reviewer_name = review.find(\"span\", class_=\"a-profile-name\")\n",
    "        reviewer_name = reviewer_name.get_text(strip=True) if reviewer_name else \"Anonymous\"\n",
    "        \n",
    "        review_title = review.find(\"a\", {\"data-hook\": \"review-title\"})\n",
    "        review_title = review_title.get_text(strip=True) if review_title else \"No title\"\n",
    "        \n",
    "        review_date = review.find(\"span\", {\"data-hook\": \"review-date\"})\n",
    "        review_date = review_date.get_text(strip=True) if review_date else \"No date\"\n",
    "        \n",
    "        review_rating = review.find(\"i\", {\"data-hook\": \"review-star-rating\"})\n",
    "        review_rating = review_rating.get_text(strip=True) if review_rating else \"No rating\"\n",
    "        \n",
    "        review_text = review.find(\"span\", {\"data-hook\": \"review-body\"})\n",
    "        review_text = review_text.get_text(strip=True) if review_text else \"No review text\"\n",
    "        \n",
    "        helpful_votes = review.find(\"span\", {\"data-hook\": \"helpful-vote-statement\"})\n",
    "        helpful_votes = helpful_votes.get_text(strip=True) if helpful_votes else \"0 helpful votes\"\n",
    "        \n",
    "        reviews.append({\n",
    "            \"Reviewer Name\": reviewer_name,\n",
    "            \"Review Title\": review_title,\n",
    "            \"Review Date\": review_date,\n",
    "            \"Rating\": review_rating,\n",
    "            \"Review Text\": review_text,\n",
    "            \"Helpful Votes\": helpful_votes,\n",
    "        })\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "def scrape_amazon_product(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the webpage\")\n",
    "        return None, None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract product details\n",
    "    product_details = extract_product_details(soup)\n",
    "    \n",
    "    # Extract reviews\n",
    "    reviews = extract_reviews(soup)\n",
    "    \n",
    "    return product_details, reviews\n",
    "\n",
    "def save_to_csv(product_details, reviews):\n",
    "    if product_details:\n",
    "        # Convert product details into DataFrame and save to CSV\n",
    "        product_df = pd.DataFrame([product_details])\n",
    "        product_filename = \"product_details.csv\"\n",
    "        product_df.to_csv(product_filename, index=False)\n",
    "        print(f\"Product details saved to {product_filename}\")\n",
    "    \n",
    "    if reviews:\n",
    "        # Convert reviews into DataFrame and save to CSV\n",
    "        reviews_df = pd.DataFrame(reviews)\n",
    "        reviews_filename = \"product_reviews.csv\"\n",
    "        reviews_df.to_csv(reviews_filename, index=False)\n",
    "        print(f\"Reviews saved to {reviews_filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example Amazon Product URL\n",
    "    product_url = \"https://amzn.in/d/2fYeMTZ\"\n",
    "\n",
    "    # Scrape product details and reviews\n",
    "    product_details, reviews = scrape_amazon_product(product_url)\n",
    "\n",
    "    if product_details and reviews:\n",
    "        # Save to CSV files\n",
    "        save_to_csv(product_details, reviews)\n",
    "    else:\n",
    "        print(\"No data to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc943855-71f4-497f-a715-12a0b72104bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to product_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def extract_product_details(soup):\n",
    "    # Extract product name, price, description, image URL, and rating\n",
    "    product_name = soup.find(\"span\", {\"id\": \"productTitle\"})\n",
    "    product_name = product_name.get_text(strip=True) if product_name else \"No product name available\"\n",
    "    \n",
    "    price = soup.find(\"span\", {\"class\": \"a-price-whole\"})\n",
    "    price = price.get_text(strip=True) if price else \"Not Available\"\n",
    "    \n",
    "    img = soup.find(\"img\", {\"id\": \"landingImage\"})\n",
    "    img_url = img['src'] if img else \"No image found\"\n",
    "    \n",
    "    description = soup.find(\"div\", {\"id\": \"feature-bullets\"})\n",
    "    description = description.get_text(strip=True) if description else \"No description available\"\n",
    "    \n",
    "    rating = soup.find(\"span\", {\"class\": \"a-icon-alt\"})\n",
    "    rating = rating.get_text(strip=True) if rating else \"No rating\"\n",
    "\n",
    "    return {\n",
    "        \"Product Name\": product_name,\n",
    "        \"Price\": price,\n",
    "        \"Image URL\": img_url,\n",
    "        \"Description\": description,\n",
    "        \"Overall Rating\": rating,\n",
    "    }\n",
    "\n",
    "def extract_reviews(soup):\n",
    "    # Extract all review texts\n",
    "    reviews = []\n",
    "    review_elements = soup.find_all(\"li\", {\"data-hook\": \"review\"})\n",
    "    for review in review_elements:\n",
    "        review_text = review.find(\"span\", {\"data-hook\": \"review-body\"})\n",
    "        if review_text:\n",
    "            reviews.append(review_text.get_text(strip=True))  # Append only valid review texts\n",
    "    return reviews\n",
    "\n",
    "def scrape_amazon_product(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract product details\n",
    "    product_details = extract_product_details(soup)\n",
    "    \n",
    "    # Extract review texts\n",
    "    reviews = extract_reviews(soup)\n",
    "    \n",
    "    # Combine product details and reviews into one dictionary\n",
    "    for i, review_text in enumerate(reviews, start=1):\n",
    "        product_details[f\"Review Text {i}\"] = review_text\n",
    "    \n",
    "    return product_details\n",
    "\n",
    "def save_to_csv(product_data):\n",
    "    if product_data:\n",
    "        # Convert product data into DataFrame and save to CSV\n",
    "        product_df = pd.DataFrame([product_data])\n",
    "        filename = \"product_data.csv\"\n",
    "        product_df.to_csv(filename, index=False)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example Amazon Product URL\n",
    "    product_url = \"https://amzn.in/d/g5fNUYy\"\n",
    "\n",
    "    # Scrape product details and reviews\n",
    "    product_data = scrape_amazon_product(product_url)\n",
    "\n",
    "    if product_data:\n",
    "        # Save to a single CSV file\n",
    "        save_to_csv(product_data)\n",
    "    else:\n",
    "        print(\"No data to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e484069b-c759-4bc0-9dc8-bba95c3a7d57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
